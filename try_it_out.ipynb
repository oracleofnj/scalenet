{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.custom_transformers as custom_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackDCTs(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_bases(length, num_bases):\n",
    "        xs = torch.tensor(range(length)).type(torch.FloatTensor)\n",
    "        bases = [\n",
    "            torch.cos(np.pi * p * (2. * xs + 1) / (2 * length))\n",
    "            for p in range(num_bases)\n",
    "        ]\n",
    "\n",
    "        def mesh_bases(b1, b2):\n",
    "            rr, cc = torch.meshgrid([b1, b2])\n",
    "            return rr * cc\n",
    "\n",
    "        full_bases = torch.stack([\n",
    "            mesh_bases(b1, b2)\n",
    "            for b1 in bases\n",
    "            for b2 in bases\n",
    "        ])\n",
    "        return full_bases\n",
    "\n",
    "    def __init__(self, num_bases, lengths):\n",
    "        super(StackDCTs, self).__init__()\n",
    "        self.num_bases = num_bases\n",
    "        self.lengths = lengths.copy()\n",
    "        for length in self.lengths:\n",
    "            buffer_name = 'basis_convolution_weights_{0}'.format(length)\n",
    "            self.register_buffer(\n",
    "                buffer_name,\n",
    "                StackDCTs.make_bases(length, num_bases).repeat(3,1,1).unsqueeze(1)\n",
    "            )\n",
    "            \n",
    "    def forward(self, minibatch):\n",
    "        scales = []\n",
    "        for length in self.lengths:\n",
    "            buffer_name = 'basis_convolution_weights_{0}'.format(length)\n",
    "            repeated_bases = self.state_dict()[buffer_name]\n",
    "            left_padding = (repeated_bases.shape[-1] - 1) // 2\n",
    "            right_padding = repeated_bases.shape[-1] - 1 - left_padding\n",
    "            top_padding = (repeated_bases.shape[-2] - 1) // 2\n",
    "            bottom_padding = repeated_bases.shape[-2] - 1 - top_padding\n",
    "            minibatch_padded = F.pad(\n",
    "                minibatch,\n",
    "                (left_padding, right_padding, top_padding, bottom_padding)\n",
    "            )\n",
    "            scales.append(F.conv2d(\n",
    "                input=minibatch_padded,\n",
    "                weight=repeated_bases,\n",
    "                groups=3\n",
    "            ))\n",
    "        return torch.stack(scales, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.stack = StackDCTs(3, [3, 4, 6, 8])\n",
    "        self.pool_flat = nn.MaxPool3d(\n",
    "            kernel_size=(1, 2, 2),\n",
    "            stride=(1, 2, 2)\n",
    "        )\n",
    "        self.pool_deep = nn.MaxPool3d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # Size is (n, 3 * (3 * 3), 4, 32, 32)\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels=3 * (3*3),\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3 = nn.Conv3d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv4 = nn.Conv3d(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2 * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stack(x)\n",
    "        x = self.pool_flat(F.relu(self.conv1(x)))\n",
    "        x = self.pool_flat(F.relu(self.conv2(x)))\n",
    "        x = self.pool_flat(F.relu(self.conv3(x)))\n",
    "        x = self.pool_deep(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 256 * 2 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_ycbcr = torchvision.datasets.CIFAR10(\n",
    "    root='./image_files',\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),\n",
    "        custom_transformers.ToYCbYr()\n",
    "    ])\n",
    ")\n",
    "trainloader_ycbcr = torch.utils.data.DataLoader(trainset_ycbcr, batch_size=16, shuffle=True, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.168\n",
      "[1,   200] loss: 2.012\n",
      "[1,   300] loss: 1.922\n",
      "[1,   400] loss: 1.875\n",
      "[1,   500] loss: 1.863\n",
      "[1,   600] loss: 1.828\n",
      "[1,   700] loss: 1.781\n",
      "[1,   800] loss: 1.727\n",
      "[1,   900] loss: 1.690\n",
      "[1,  1000] loss: 1.679\n",
      "[1,  1100] loss: 1.701\n",
      "[1,  1200] loss: 1.609\n",
      "[1,  1300] loss: 1.606\n",
      "[1,  1400] loss: 1.552\n",
      "[1,  1500] loss: 1.565\n",
      "[1,  1600] loss: 1.633\n",
      "[1,  1700] loss: 1.625\n",
      "[1,  1800] loss: 1.563\n",
      "[1,  1900] loss: 1.590\n",
      "[1,  2000] loss: 1.556\n",
      "[1,  2100] loss: 1.604\n",
      "[1,  2200] loss: 1.549\n",
      "[1,  2300] loss: 1.489\n",
      "[1,  2400] loss: 1.547\n",
      "[1,  2500] loss: 1.507\n",
      "[1,  2600] loss: 1.530\n",
      "[1,  2700] loss: 1.522\n",
      "[1,  2800] loss: 1.621\n",
      "[1,  2900] loss: 1.490\n",
      "[1,  3000] loss: 1.497\n",
      "[1,  3100] loss: 1.498\n",
      "[2,   100] loss: 1.499\n",
      "[2,   200] loss: 1.420\n",
      "[2,   300] loss: 1.430\n",
      "[2,   400] loss: 1.447\n",
      "[2,   500] loss: 1.471\n",
      "[2,   600] loss: 1.441\n",
      "[2,   700] loss: 1.445\n",
      "[2,   800] loss: 1.424\n",
      "[2,   900] loss: 1.481\n",
      "[2,  1000] loss: 1.528\n",
      "[2,  1100] loss: 1.425\n",
      "[2,  1200] loss: 1.441\n",
      "[2,  1300] loss: 1.483\n",
      "[2,  1400] loss: 1.432\n",
      "[2,  1500] loss: 1.430\n",
      "[2,  1600] loss: 1.412\n",
      "[2,  1700] loss: 1.428\n",
      "[2,  1800] loss: 1.404\n",
      "[2,  1900] loss: 1.431\n",
      "[2,  2000] loss: 1.457\n",
      "[2,  2100] loss: 1.453\n",
      "[2,  2200] loss: 1.462\n",
      "[2,  2300] loss: 1.435\n",
      "[2,  2400] loss: 1.422\n",
      "[2,  2500] loss: 1.412\n",
      "[2,  2600] loss: 1.426\n",
      "[2,  2700] loss: 1.422\n",
      "[2,  2800] loss: 1.407\n",
      "[2,  2900] loss: 1.426\n",
      "[2,  3000] loss: 1.464\n",
      "[2,  3100] loss: 1.486\n",
      "[3,   100] loss: 1.332\n",
      "[3,   200] loss: 1.371\n",
      "[3,   300] loss: 1.415\n",
      "[3,   400] loss: 1.344\n",
      "[3,   500] loss: 1.402\n",
      "[3,   600] loss: 1.385\n",
      "[3,   700] loss: 1.452\n",
      "[3,   800] loss: 1.380\n",
      "[3,   900] loss: 1.429\n",
      "[3,  1000] loss: 1.420\n",
      "[3,  1100] loss: 1.486\n",
      "[3,  1200] loss: 1.405\n",
      "[3,  1300] loss: 1.444\n",
      "[3,  1400] loss: 1.401\n",
      "[3,  1500] loss: 1.403\n",
      "[3,  1600] loss: 1.476\n",
      "[3,  1700] loss: 1.449\n",
      "[3,  1800] loss: 1.464\n",
      "[3,  1900] loss: 1.444\n",
      "[3,  2000] loss: 1.462\n",
      "[3,  2100] loss: 1.426\n",
      "[3,  2200] loss: 1.450\n",
      "[3,  2300] loss: 1.471\n",
      "[3,  2400] loss: 1.483\n",
      "[3,  2500] loss: 1.472\n",
      "[3,  2600] loss: 1.436\n",
      "[3,  2700] loss: 1.460\n",
      "[3,  2800] loss: 1.543\n",
      "[3,  2900] loss: 1.502\n",
      "[3,  3000] loss: 1.385\n",
      "[3,  3100] loss: 1.413\n",
      "[4,   100] loss: 1.385\n",
      "[4,   200] loss: 1.439\n",
      "[4,   300] loss: 1.408\n",
      "[4,   400] loss: 1.456\n",
      "[4,   500] loss: 1.431\n",
      "[4,   600] loss: 1.479\n",
      "[4,   700] loss: 1.466\n",
      "[4,   800] loss: 1.423\n",
      "[4,   900] loss: 1.474\n",
      "[4,  1000] loss: 1.452\n",
      "[4,  1100] loss: 1.553\n",
      "[4,  1200] loss: 1.504\n",
      "[4,  1300] loss: 1.479\n",
      "[4,  1400] loss: 1.503\n",
      "[4,  1500] loss: 1.541\n",
      "[4,  1600] loss: 1.541\n",
      "[4,  1700] loss: 1.494\n",
      "[4,  1800] loss: 1.504\n",
      "[4,  1900] loss: 1.457\n",
      "[4,  2000] loss: 1.527\n",
      "[4,  2100] loss: 1.562\n",
      "[4,  2200] loss: 1.513\n",
      "[4,  2300] loss: 1.512\n",
      "[4,  2400] loss: 1.545\n",
      "[4,  2500] loss: 1.582\n",
      "[4,  2600] loss: 1.494\n",
      "[4,  2700] loss: 1.503\n",
      "[4,  2800] loss: 1.551\n",
      "[4,  2900] loss: 1.471\n",
      "[4,  3000] loss: 1.499\n",
      "[4,  3100] loss: 1.621\n",
      "[5,   100] loss: 1.500\n",
      "[5,   200] loss: 1.534\n",
      "[5,   300] loss: 1.518\n",
      "[5,   400] loss: 1.525\n",
      "[5,   500] loss: 1.521\n",
      "[5,   600] loss: 1.571\n",
      "[5,   700] loss: 1.494\n",
      "[5,   800] loss: 1.507\n",
      "[5,   900] loss: 1.536\n",
      "[5,  1000] loss: 1.491\n",
      "[5,  1100] loss: 1.597\n",
      "[5,  1200] loss: 1.574\n",
      "[5,  1300] loss: 1.577\n",
      "[5,  1400] loss: 1.550\n",
      "[5,  1500] loss: 1.466\n",
      "[5,  1600] loss: 1.513\n",
      "[5,  1700] loss: 1.502\n",
      "[5,  1800] loss: 1.539\n",
      "[5,  1900] loss: 1.580\n",
      "[5,  2000] loss: 1.470\n",
      "[5,  2100] loss: 1.537\n",
      "[5,  2200] loss: 1.527\n",
      "[5,  2300] loss: 1.531\n",
      "[5,  2400] loss: 1.585\n",
      "[5,  2500] loss: 1.547\n",
      "[5,  2600] loss: 1.569\n",
      "[5,  2700] loss: 1.552\n",
      "[5,  2800] loss: 1.554\n",
      "[5,  2900] loss: 1.609\n",
      "[5,  3000] loss: 1.566\n",
      "[5,  3100] loss: 1.537\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader_ycbcr, 0):\n",
    "        # get the inputs\n",
    "        inputs_cpu, labels_cpu = data\n",
    "        inputs, labels = inputs_cpu.cuda(), labels_cpu.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./image_files', train=False,\n",
    "                                       download=False,     transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),\n",
    "        custom_transformers.ToYCbYr()\n",
    "    ])\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images_cpu, labels_cpu = data\n",
    "        images, labels = images_cpu.cuda(), labels_cpu.cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 40 %\n",
      "Accuracy of   car : 52 %\n",
      "Accuracy of  bird : 21 %\n",
      "Accuracy of   cat : 47 %\n",
      "Accuracy of  deer : 29 %\n",
      "Accuracy of   dog : 46 %\n",
      "Accuracy of  frog : 54 %\n",
      "Accuracy of horse : 81 %\n",
      "Accuracy of  ship : 44 %\n",
      "Accuracy of truck : 85 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images_cpu, labels_cpu = data\n",
    "        images, labels = images_cpu.cuda(), labels_cpu.cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
